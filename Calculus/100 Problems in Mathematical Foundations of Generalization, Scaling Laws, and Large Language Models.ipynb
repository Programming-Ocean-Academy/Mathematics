{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTI//2gG6kBaHx0j6rkmx/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Phase 10 — Mathematical Foundations of Generalization, Scaling Laws, and Large Language Models (100 Problems)\n","\n","---\n","\n","## Module 1 — Classical Generalization Theory (VC/Rademacher/Uniform Convergence) (1–10)\n","\n","1. Prove a VC generalization bound for binary classifiers with VC-dimension $$d.$$\n","2. Compute VC-dimension of intervals on $$\\mathbb{R}.$$\n","3. Compute VC-dimension of linear separators in $$\\mathbb{R}^d.$$\n","4. Derive empirical Rademacher complexity for linear class with $$\\|w\\|\\le B.$$\n","5. Bound generalization gap using Rademacher complexity for Lipschitz loss.\n","6. Show uniform convergence bound for finite hypothesis class with $$|H|=N.$$\n","7. Prove Massart’s finite class lemma and apply to linear separators.\n","8. Derive margin-based generalization bound for SVM with hinge loss.\n","9. Show relation between covering numbers and Rademacher complexity.\n","10. Compare bounds from VC vs Rademacher on a synthetic dataset.\n","\n","---\n","\n","## Module 2 — PAC-Bayes & Algorithmic Stability (11–20)\n","\n","11. Derive a PAC-Bayes bound with KL divergence $$KL(Q\\|P).$$\n","12. Apply PAC-Bayes bound to Gaussian prior/posterior over linear models.\n","13. Optimize PAC-Bayes objective over posterior variance.\n","14. Prove algorithmic stability bound for ERM with strongly convex loss.\n","15. Derive stability of ridge regression and its generalization bound.\n","16. Show stability of SGD under Lipschitz-smooth, strongly convex objective.\n","17. Compare PAC-Bayes vs stability bounds on the same model.\n","18. Relate flat minima (Hessian trace) to PAC-Bayes via $$\\log\\det$$ terms.\n","19. Compute a data-dependent PAC-Bayes bound with empirical Fisher.\n","20. Tighten bounds using localized Rademacher complexity.\n","\n","---\n","\n","## Module 3 — Double Descent, Implicit Bias & Overparameterization (21–30)\n","\n","21. Simulate risk vs width to exhibit double descent.\n","22. Show interpolation threshold in least-squares with $$n<d.$$\n","23. Prove minimum-norm interpolating solution for underdetermined linear regression.\n","24. Derive gradient flow solution and its implicit regularization.\n","25. Show GD converges to max-margin classifier in separable logistic regression.\n","26. Analyze effect of label noise on double descent.\n","27. Compute bias–variance decomposition pre/post interpolation.\n","28. Compare early stopping vs explicit $$\\ell_2$$ regularization.\n","29. Relate flatness (small Hessian eigenvalues) to generalization empirically.\n","30. Show role of weight decay as Tikhonov regularization.\n","\n","---\n","\n","## Module 4 — Scaling Laws & Data–Model–Compute Tradeoffs (31–40)\n","\n","31. Fit power-law $$L(N)=aN^{-b}+c$$ to loss vs data size $$N.$$\n","32. Estimate scaling exponent for loss vs parameter count $$P.$$\n","33. Joint scaling: fit loss vs $$(P,D,C)$$ with log–log regression.\n","34. Optimize compute allocation between data and parameters under a budget.\n","35. Derive optimal training tokens for fixed model size.\n","36. Compare scaling exponents across architectures (MLP vs Transformer).\n","37. Extrapolate loss at $$10\\times$$ compute using fitted scaling law.\n","38. Quantify irreducible loss floor $$c$$ and its effect on returns to scale.\n","39. Analyze sensitivity of exponents to tokenizer vocabulary size.\n","40. Compute Pareto frontier of (loss, compute) for several models.\n","\n","---\n","\n","## Module 5 — Information Theory of Sequence Modeling (41–50)\n","\n","41. Show $$\\text{Perplexity} = \\exp(H)$$ where $$H$$ is cross-entropy per token.\n","42. Estimate entropy rate of a text source from samples.\n","43. Compute mutual information $$I(X_{1:t-1};X_t)$$ in a Markov model.\n","44. Derive cross-entropy gap between model and data distribution.\n","45. Connect KL divergence to excess risk in language modeling.\n","46. Compute bits-back argument for compression with generative models.\n","47. Analyze effect of context length on conditional entropy.\n","48. Show diminishing returns of longer context via mutual information decay.\n","49. Compare tokenization schemes by induced entropy per subword.\n","50. Derive relation between calibration error and log-likelihood.\n","\n","---\n","\n","## Module 6 — Transformers: Capacity, Expressivity, and Training Dynamics (51–65)\n","\n","51. Prove a universal approximation result for attention on finite sequences.\n","52. Bound Lipschitz constant of a Transformer block (attn + MLP + residual).\n","53. Compute spectral norm bound for multi-head attention weight matrices.\n","54. Derive gradient of attention logits and analyze saturation.\n","55. Show softmax temperature’s effect on gradient scale.\n","56. Analyze depth vs width tradeoff for expressivity with residual connections.\n","57. Derive conditions preventing attention collapse (all mass to one token).\n","58. Compute Jacobian conditioning across stacked layers.\n","59. Analyze layer norm’s effect on gradient flow.\n","60. Derive learning rate scaling with model width.\n","61. Show linearized training dynamics (NTK) for a shallow Transformer.\n","62. Compare pre-LN vs post-LN blocks for stability (theoretical criterion).\n","63. Prove convergence of training under smoothness/Lipschitz assumptions.\n","64. Evaluate curvature (HVP) along training and relate to step size.\n","65. Analyze gradient noise scale vs batch size during pretraining.\n","\n","---\n","\n","## Module 7 — Pretraining, Fine-Tuning, In-Context Learning & Emergence (66–80)\n","\n","66. Model pretraining as risk minimization on mixture distributions.\n","67. Quantify transfer improvement from pretraining via linear probe accuracy.\n","68. Derive effective learning rate change under LoRA adaptation.\n","69. Analyze rank-constrained updates and generalization.\n","70. Compute forgetting in continual fine-tuning (Fisher overlap).\n","71. Derive meta-gradient for in-context learning in linear models.\n","72. Show emergent linear regression in attention with key–value design.\n","73. Quantify emergence thresholds for new capabilities via scaling.\n","74. Analyze grokking as phase transition in train vs test loss dynamics.\n","75. Derive conditions for few-shot generalization with tokenizer bias.\n","76. Measure calibration shift after instruction tuning.\n","77. Compare RLHF objective with KL-regularized policy optimization.\n","78. Derive optimal KL penalty to preserve pretraining distribution.\n","79. Analyze safety–helpfulness tradeoff via multi-objective optimization.\n","80. Quantify in-context learning capacity vs context window length.\n","\n","---\n","\n","## Module 8 — Evaluation, Calibration, Uncertainty & Robustness (81–100)\n","\n","81. Compute Brier score and relate to negative log-likelihood.\n","82. Estimate expected calibration error (ECE) with binning.\n","83. Optimize temperature scaling to minimize NLL on a validation set.\n","84. Derive ensemble log-likelihood improvement vs single model.\n","85. Compute epistemic vs aleatoric uncertainty decomposition.\n","86. Analyze OOD detection via likelihood ratio tests.\n","87. Derive conformal prediction intervals for sequence probabilities.\n","88. Show robustness curve under adversarial perturbations of tokens.\n","89. Evaluate coverage vs sharpness for uncertainty estimates.\n","90. Calibrate beam search scores to match true sequence probabilities.\n","91. Compare perplexity with downstream task risk via Bayes decision rule.\n","92. Derive regret bounds for next-token prediction under misspecification.\n","93. Quantify distribution shift via population stability index on tokens.\n","94. Evaluate effect of quantization on perplexity and calibration.\n","95. Compute generalization gap across domains (books → code → dialogue).\n","96. Derive optimal early-stopping rule from validation log-loss trend.\n","97. Bound catastrophic forgetting using Fisher-based quadratic penalty.\n","98. Analyze RLHF-induced distribution shift on token-level statistics.\n","99. Evaluate selective prediction (abstention) under risk constraints.\n","100. Build a composite score aggregating perplexity, ECE, robustness, and cost.\n","\n"],"metadata":{"id":"DBQYC6Pp-Y5D"}}]}