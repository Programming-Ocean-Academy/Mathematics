{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqOJZ/nV22Q9J6Z6k0XqqU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PHASE 9 — Mathematical Physics of Deep Learning (100 Problems)\n","\n","---\n","\n","## Module 1 — Energy Landscapes & Thermodynamics (1–15)\n","\n","1. Show that gradient descent minimizes an energy functional $$E(\\theta).$$\n","2. Derive the energy dissipation identity: $$\\frac{dE}{dt} = -\\|\\nabla E\\|^2.$$\n","3. Compute stationary points of an energy landscape $$E(x,y)=x^4-3x^2+y^2.$$\n","4. Define basins of attraction for a multi-well potential.\n","5. Show connection between potential wells and local minima.\n","6. Derive Gibbs distribution $$p(x) \\propto e^{-E(x)/T}.$$\n","7. Evaluate partition function $$Z = \\int e^{-E(x)}\\, dx$$ for $$E(x)=x^2.$$\n","8. Compute free energy $$F = -T\\log Z$$ for Gaussian potential.\n","9. Derive entropy $$S = -\\int p(x)\\log p(x)\\,dx.$$\n","10. Compute free energy gradient for variational distribution $$q(x).$$\n","11. Illustrate high-dimensional “needle in a haystack” phenomenon.\n","12. Show that deep networks’ loss surface has many flat minima.\n","13. Prove that sharp minima correspond to large Hessian eigenvalues.\n","14. Compare flat vs sharp minima using Hessian spectrum.\n","15. Plot energy landscape evolution during training.\n","\n","---\n","\n","## Module 2 — Statistical Mechanics of Learning (16–30)\n","\n","16. Define microscopic (parameters) vs macroscopic (loss/accuracy) variables.\n","17. Derive Boltzmann distribution for model weights under noise.\n","18. Show SGD $$\\approx$$ Langevin dynamics: $$d\\theta = -\\nabla L\\, dt + \\sqrt{2T}\\, dW_t.$$\n","19. Derive Fokker–Planck equation for weight distribution.\n","20. Solve Fokker–Planck for quadratic loss.\n","21. Compute stationary density for stochastic gradient Langevin dynamics.\n","22. Compare deterministic GD vs stochastic thermally driven dynamics.\n","23. Analyze escape time from local minimum using Kramers’ rate.\n","24. Derive large deviations rate for escape from energy well.\n","25. Analyze SGD noise scaling with batch size.\n","26. Derive effective temperature $$T \\propto \\eta \\frac{\\sigma^2}{B}.$$\n","27. Show temperature controls exploration vs convergence.\n","28. Analyze phase transition at critical batch size.\n","29. Compute equilibrium distribution for 1D learning rule.\n","30. Compare SGD trajectory to diffusion in potential wells.\n","\n","---\n","\n","## Module 3 — Mean-Field Theory & Neural Tangent Kernel (31–45)\n","\n","31. Derive mean-field limit of neural network output as width $$\\to \\infty.$$\n","32. Compute covariance kernel of random initialization.\n","33. Define NTK $$K(x,x')$$ for 2-layer network.\n","34. Show NTK remains constant during training in infinite width.\n","35. Compute prediction dynamics $$f_t = f_0 - K(t)\\nabla L.$$\n","36. Derive linearized training dynamics around initialization.\n","37. Compute NTK Gram matrix for sample dataset.\n","38. Show convergence under positive-definite NTK.\n","39. Compare NTK vs GP view of infinite-width networks.\n","40. Compute signal propagation through infinite-depth network.\n","41. Show weight distribution becomes Gaussian via CLT.\n","42. Derive recurrence for variance preservation across layers.\n","43. Analyze exploding/vanishing signal via Jacobian.\n","44. Show critical initialization condition for stable propagation.\n","45. Compute mean-field dynamics of gradient descent in 1-layer network.\n","\n","---\n","\n","## Module 4 — Information Theory & Generalization (46–60)\n","\n","46. Compute mutual information $$I(X;Y)$$ for binary channel.\n","47. Derive entropy of Gaussian $$X \\sim \\mathcal{N}(0,\\sigma^2).$$\n","48. Compute KL divergence $$D_{KL}(p\\|q)$$ for two Gaussians.\n","49. Derive PAC-Bayes generalization bound.\n","50. Compute PAC-Bayes bound for small variance prior.\n","51. Analyze information bottleneck Lagrangian.\n","52. Derive IB objective: $$L = I(X;Z) - \\beta\\, I(Z;Y).$$\n","53. Compute gradient of the IB objective.\n","54. Estimate mutual information via Monte Carlo.\n","55. Show flat minima correspond to low-information weights.\n","56. Relate Hessian trace to effective information capacity.\n","57. Compute complexity penalty via log-det Hessian.\n","58. Derive “sharpness” metric for a model.\n","59. Compare generalization in flat vs sharp minima.\n","60. Show compression (implicit SGD regularization) improves generalization.\n","\n","---\n","\n","## Module 5 — Neural Dynamics as Differential Equations (61–75)\n","\n","61. Show continuous-time gradient flow ODE: $$\\dot{\\theta}=-\\nabla L.$$\n","62. Solve gradient flow for quadratic loss.\n","63. Derive stability condition for linear ODE $$\\dot{x}=Ax.$$\n","64. Apply stability analysis to weight updates.\n","65. Model deep network as composition of flows.\n","66. Show residual network approximates ODE: $$\\dot{x}=f(x,t).$$\n","67. Derive backpropagation as adjoint differential equation.\n","68. Compute adjoint dynamics for scalar ODE.\n","69. Derive adjoint equation for neural ODE block.\n","70. Compute numerical solution using Euler discretization.\n","71. Compare Euler vs RK4 for feature dynamics.\n","72. Compute Lipschitz constant of a residual block.\n","73. Analyze stability of deep networks under Lipschitz constraint.\n","74. Derive continuous depth parameterization in Neural ODEs.\n","75. Compute flow map $$x(T)=\\Phi_T(x(0))$$ for simple vector field.\n","\n","---\n","\n","## Module 6 — Diffusion Models & Stochastic Processes (76–90)\n","\n","76. Define forward diffusion SDE: $$dx = \\sqrt{\\beta_t}\\, dW_t.$$\n","77. Derive reverse-time SDE from score function.\n","78. Compute score $$\\nabla_x \\log p_t(x)$$ for Gaussian.\n","79. Derive denoising score matching objective.\n","80. Compute KL divergence between diffusion transitions.\n","81. Derive loss for DDPM forward process.\n","82. Compute analytic reverse kernel for Gaussian forward noise.\n","83. Show connection between score matching and denoising.\n","84. Simulate 1D diffusion forward process.\n","85. Simulate reverse denoising step.\n","86. Derive continuous-time diffusion ODE.\n","87. Connection between diffusion ODE and probability flow ODE.\n","88. Compute trace Jacobian for CNF (Hutchinson estimator).\n","89. Derive ELBO for diffusion model.\n","90. Compare diffusion vs energy-based vs flow models.\n","\n","---\n","\n","## Module 7 — Geometry of Attention & Transformer Dynamics (91–100)\n","\n","91. Compute Jacobian of attention output w.r.t. query vector.\n","92. Analyze stability of multi-head attention via spectral norm.\n","93. Derive Lipschitz condition for attention layer.\n","94. Compute Fisher information for attention weights.\n","95. Analyze curvature (Hessian) of attention logits.\n","96. Derive capacity scaling law for attention depth.\n","97. Show residual stream forms a dynamical system.\n","98. Compute energy function for attention update.\n","99. Show how layer normalization stabilizes gradient dynamics.\n","100. Connect Transformer depth $$\\to$$ continuous dynamical flow model.\n"],"metadata":{"id":"O_N9qD5j9CiN"}}]}