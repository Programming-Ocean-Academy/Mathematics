{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWX99N75TWC0lsDFJqe0qj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Applied Calculus in Machine Learning & Optimization (Phase 7)\n","\n","This phase transforms your analytical mastery into *computational intelligence*.  \n","It covers how gradients, derivatives, and integrals become the engines of learning — structured into **6 modules** and **100 core problems / experiments**.\n","\n","---\n","\n","## **Module 1 — Gradient Mechanics & Cost Landscapes (1–15)**\n","\n","1. Compute gradient of $$L(w)=\\frac{1}{2}(y-\\hat{y})^2.$$\n","2. Compute gradient for $$L(w)=|Xw-y|^2.$$\n","3. Derive update rule $$w_{t+1}=w_t-\\eta\\nabla L(w_t).$$\n","4. Compute gradient of $$f(x)=x^4-3x^3+2x$$ at $$x=1,2,3.$$\n","5. Visualize cost surface $$f(x,y)=x^2+y^2.$$\n","6. Compute contour lines and direction of steepest descent.\n","7. Show gradient ⟂ contour for $$f(x,y)=x^2+y^2.$$\n","8. Approximate gradient numerically via finite differences.\n","9. Implement gradient check for neural loss $$L(w)=|Xw-y|^2.$$\n","10. Show gradient sign controls learning direction.\n","11. Compute Hessian $$H=\\nabla^2f$$ for $$f=x^2+xy+y^2.$$\n","12. Identify convex vs non-convex shapes via eigenvalues of $$H.$$\n","13. Plot cost function $$f(x)=x^2$$ and $$f(x)=x^4-3x^3+2$$ to contrast convexity.\n","14. Demonstrate gradient descent convergence on $$f(x)=x^2.$$\n","15. Show divergence if $$\\eta$$ (learning rate) is too large.\n","\n","---\n","\n","## **Module 2 — Optimization Algorithms (16–35)**\n","\n","16. Derive SGD update for linear regression.  \n","17. Derive batch gradient descent and compare.  \n","18. Implement momentum update: $$v_t=\\beta v_{t-1}+(1-\\beta)\\nabla L.$$  \n","19. Simulate overshooting and damping in momentum.  \n","20. RMSProp: $$s_t=\\rho s_{t-1}+(1-\\rho)\\nabla^2L.$$  \n","21. Adam optimizer: $(m_t, v_t)$ updates and bias correction.  \n","22. Compare convergence of SGD vs Adam.  \n","23. Use contour plots to visualize optimizer paths.  \n","24. Show adaptive learning rates reduce oscillations.  \n","25. Compute gradient clipping for exploding gradients.  \n","26. Derive learning-rate schedule $$\\eta_t=\\frac{\\eta_0}{1+kt}.$$  \n","27. Optimize $$f(x,y)=x^2+3y^2$$ with different $$\\eta.$$  \n","28. Analyze local minima in $$f(x)=x^4-3x^3+2.$$  \n","29. Demonstrate saddle point in $$f(x,y)=x^2-y^2.$$  \n","30. Show escaping saddle with noise (SGD stochasticity).  \n","31. Derive condition for convergence in convex quadratic.  \n","32. Compute gradient norm threshold stopping rule.  \n","33. Plot loss vs iteration for different optimizers.  \n","34. Measure computational cost per iteration.  \n","35. Summarize optimizer trade-offs.\n","\n","---\n","\n","## **Module 3 — Jacobians, Chain Rule & Backpropagation (36–55)**\n","\n","36. Compute Jacobian of $$f(x,y)=(x^2+y,\\,xy).$$  \n","37. Find Jacobian for softmax: $$s_i=\\frac{e^{z_i}}{\\sum_j e^{z_j}}.$$  \n","38. Derive chain rule for composite $$f(g(h(x))).$$  \n","39. Compute $$\\frac{dy}{dx}$$ for $$y=\\sigma(wx+b).$$  \n","40. Differentiate ReLU, sigmoid, tanh functions.  \n","41. Derive backprop rule for one-layer perceptron.  \n","42. Derive gradient of cross-entropy loss $$L=-y\\ln\\hat{y}-(1-y)\\ln(1-\\hat{y}).$$  \n","43. Backprop through two-layer network $$a_2=W_2\\sigma(W_1x+b_1)+b_2.$$  \n","44. Compute gradient of MSE loss wrt $$(W_1,W_2).$$  \n","45. Implement numerical gradient check.  \n","46. Compute Jacobian of convolution output wrt kernel weights.  \n","47. Backprop through pooling (max, avg).  \n","48. Chain rule through normalization layer.  \n","49. Compute gradient of batch-norm output wrt input.  \n","50. Backprop through residual block $$y=x+f(x).$$  \n","51. Show how vector calculus ($\\nabla$) operator underlies backprop.  \n","52. Compute elementwise gradient for matrix multiplication $$C=AB.$$  \n","53. Derive gradient for softmax cross-entropy in matrix form.  \n","54. Differentiate scalar loss wrt tensor using Einstein summation.  \n","55. Visualize computational graph and gradient flow.\n","\n","---\n","\n","## **Module 4 — Optimization Landscapes & Loss Geometry (56–75)**\n","\n","56. Analyze gradient field for $$f(x,y)=x^3-3xy^2.$$  \n","57. Find critical points and classify via Hessian eigenvalues.  \n","58. Plot gradient flow trajectories.  \n","59. Examine vanishing/exploding gradients in deep chain $$y=\\sigma^n(x).$$  \n","60. Derive gradient scaling through sigmoid composition.  \n","61. Analyze non-convex loss with multiple minima.  \n","62. Show how momentum smooths noisy landscapes.  \n","63. Compute curvature matrix $$H$$ numerically from gradients.  \n","64. Visualize loss contours for $$f(w_1,w_2)=w_1^2+100w_2^2.$$  \n","65. Simulate ill-conditioned optimization.  \n","66. Use preconditioning (normalize gradient directions).  \n","67. Demonstrate Newton’s method: $$x_{n+1}=x_n-\\frac{f'(x)}{f''(x)}.$$  \n","68. Apply Newton to $$f(x)=x^3-x-1.$$  \n","69. Compare Newton vs gradient descent convergence.  \n","70. Quasi-Newton (BFGS) update concept.  \n","71. Derive Hessian-free optimization idea.  \n","72. Compute eigen-decomposition of $$H$$ to analyze curvature.  \n","73. Implement trust-region update rule.  \n","74. Plot loss curvature before and after normalization.  \n","75. Explore plateau and saddle escape techniques.\n","\n","---\n","\n","## **Module 5 — Probabilistic & Integral Links (76–90)**\n","\n","76. Interpret integration as expectation: $$E[f(X)]=\\int f(x)p(x)\\,dx.$$  \n","77. Approximate expectation using Monte Carlo.  \n","78. Compute gradient of expected loss via sampling.  \n","79. Apply reparameterization trick for $$z=\\mu+\\sigma\\epsilon.$$  \n","80. Derive stochastic gradient of ELBO in variational autoencoder.  \n","81. Estimate area under PDF numerically.  \n","82. Integrate Gaussian kernel $$e^{-x^2}$$ via Simpson’s rule.  \n","83. Compute normalization constant of exponential family.  \n","84. Derive gradient of log-likelihood $$\\nabla_\\theta \\log p_\\theta(x).$$  \n","85. Compute Fisher information via second derivative.  \n","86. Show gradient descent ≈ maximum likelihood under quadratic loss.  \n","87. Approximate integral of sigmoid with Gaussian input.  \n","88. Use Monte Carlo integration for reinforcement-learning return.  \n","89. Compute policy-gradient estimator $$\\nabla_\\theta E[R].$$  \n","90. Apply variance reduction via baselines in gradient estimates.\n","\n","---\n","\n","## **Module 6 — Advanced Applications & Experimentation (91–100)**\n","\n","91. Derive gradient for loss $$L(W)=\\frac{1}{2N}\\sum_i|W x_i - y_i|^2.$$  \n","92. Compute numerical gradient for logistic regression cost.  \n","93. Visualize gradient descent path in 3D loss surface.  \n","94. Implement finite-difference gradient verification for CNN weight.  \n","95. Estimate gradient noise variance during SGD.  \n","96. Compute adaptive learning rate from curvature estimate.  \n","97. Apply backprop to LSTM cell gate equations.  \n","98. Derive gradient clipping impact analytically.  \n","99. Plot convergence curve for Adam vs RMSProp.  \n","100. Perform numerical experiment comparing gradient methods on Rosenbrock function $$f(x,y)=100(y-x^2)^2+(1-x)^2.$$\n","\n","---\n","\n","##  **Outcome of This Phase**\n","\n","By completing these 100 applied-AI calculus problems, you will:\n","\n","* Master **gradient descent mathematics** at the research level.  \n","* Understand **how derivatives control learning**.  \n","* Be able to **debug and design optimization algorithms**.  \n","* Move seamlessly from pure calculus to **deep-learning optimization theory**.\n"],"metadata":{"id":"wVsdHVvw2Zm7"}}]}