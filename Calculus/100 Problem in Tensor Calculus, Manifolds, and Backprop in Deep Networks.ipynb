{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORinAxRKBjurV69kxXYgmM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Phase 8: Tensor Calculus, Manifolds, and Backprop in Deep Networks\n","\n","100 problems/experiments grouped by module. No extra commentary.\n","\n","---\n","\n","## Module 1 — Matrix & Tensor Calculus Core (1–15)\n","\n","1. Compute $$\\frac{\\partial}{\\partial x}\\,(a^\\top x).$$\n","2. Compute $$\\frac{\\partial}{\\partial x}\\,(x^\\top A x)$$ for symmetric $$A.$$\n","3. Compute $$\\frac{\\partial}{\\partial A}\\,\\|Ax-b\\|_2^2.$$\n","4. Compute $$\\frac{\\partial}{\\partial W}\\,\\|XW-Y\\|_F^2.$$\n","5. Derive gradient of $$\\mathrm{tr}(A^\\top X)$$ w.r.t. $$X.$$\n","6. Compute $$\\frac{\\partial}{\\partial X}\\,\\mathrm{tr}(X^\\top B X C).$$\n","7. Derive Jacobian of $$\\mathrm{vec}(AXB)$$ w.r.t. $$\\mathrm{vec}(X).$$\n","8. Compute gradient of $$\\log\\det(X)$$ w.r.t. $$X\\succ 0.$$\n","9. Compute Hessian of $$f(x)=\\tfrac12 x^\\top Q x + c^\\top x.$$\n","10. Show $$\\nabla_X \\|X\\|_F = \\frac{X}{\\|X\\|_F}$$ for $$X\\neq 0.$$\n","11. Derive gradient/subgradient of the nuclear norm $$\\|X\\|_*$$ at full-rank $$X$$ via SVD.\n","12. Compute (sub)derivative of $$\\|WX-b\\|_1.$$\n","13. Derive $$\\nabla_x \\|Ax\\|_2 = \\frac{A^\\top A x}{\\|Ax\\|_2}.$$\n","14. Show $$\\nabla_X \\,\\mathrm{tr}(X^{-1}A)=-(X^{-\\top} A^\\top X^{-\\top}).$$\n","15. Compute $$\\frac{\\partial (u\\otimes v)}{\\partial u}$$ and $$\\frac{\\partial (u\\otimes v)}{\\partial v}.$$\n","\n","---\n","\n","## Module 2 — Automatic Differentiation & Backprop (16–30)\n","\n","16. Build computational graph for $$f(x)=\\exp(\\sin(x^2))$$; list forward/backward passes.\n","17. Backprop through $$y=\\mathrm{ReLU}(Wx+b).$$\n","18. Backprop through softmax + cross-entropy (vector form).\n","19. Backprop through batch normalization (inference mode).\n","20. Backprop through batch normalization (training mode with mean/var).\n","21. Backprop through residual block $$y=x+f(x).$$\n","22. Backprop through depthwise separable convolution.\n","23. Backprop through max-pooling (argmax mask).\n","24. Backprop through average pooling.\n","25. Backprop through layer norm.\n","26. Backprop through GELU activation.\n","27. Backprop through dropout (inverted scaling).\n","28. Compute gradient checkpointing effect on graph (symbolic).\n","29. Compare forward-mode vs reverse-mode AD for $$f:\\mathbb{R}^{d}\\to\\mathbb{R}$$ and $$f:\\mathbb{R}^{d}\\to\\mathbb{R}^d.$$\n","30. Derive vector–Jacobian product (VJP) and Jacobian–vector product (JVP) identities.\n","\n","---\n","\n","## Module 3 — Manifolds & Riemannian Geometry Basics (31–45)\n","\n","31. Define a smooth chart for the 2-sphere $$S^2$$ minus a pole; compute transition map Jacobian.\n","32. Compute metric $$g$$ on $$S^2$$ in spherical coordinates.\n","33. Derive Christoffel symbols $$\\Gamma^k_{ij}$$ for $$S^2.$$\n","34. Write geodesic equation on $$S^2.$$\n","35. Compute geodesic distance (great-circle) between two points on $$S^2.$$\n","36. Compute gradient of a scalar field $$f(\\theta,\\phi)$$ on $$S^2$$ under metric $$g.$$\n","37. Show equivalence of intrinsic gradient and Euclidean projection for $$S^2.$$\n","38. Compute volume element $$\\sqrt{\\det g}\\,d\\theta\\,d\\phi$$ on $$S^2.$$\n","39. Derive Gaussian curvature for $$S^2.$$\n","40. Define atlas for $$SO(2)$$; compute group metric from embedding.\n","41. Parameterize $$SO(3)$$ via axis–angle; compute left-invariant vector fields.\n","42. Compute exponential map $$\\exp:\\mathfrak{so}(3)\\to SO(3)$$ for a skew-symmetric matrix.\n","43. Compute logarithm map on $$SO(3)$$ for a given rotation $$R.$$\n","44. Prove geodesics on $$\\mathbb{R}^n$$ are straight lines under Euclidean metric.\n","45. Show product manifold metric for $$S^2\\times \\mathbb{R}.$$\n","\n","---\n","\n","## Module 4 — Optimization on Manifolds (46–60)\n","\n","46. Riemannian gradient of $$f(w)$$ on unit sphere constraint $$\\|w\\|=1.$$\n","47. Retraction on sphere using normalization; derive update rule.\n","48. Optimize PCA direction as maximizer of $$w^\\top \\Sigma w$$ with $$\\|w\\|=1.$$\n","49. Grassmann manifold $$\\mathrm{Gr}(k,n):$$ compute tangent projection at $$U$$ with $$U^\\top U=I.$$\n","50. Riemannian gradient for subspace fitting $$f(U)=\\|X-UU^\\top X\\|_F^2.$$\n","51. Stiefel manifold $$\\mathrm{St}(k,n):$$ derive skew-symmetric correction for gradient.\n","52. Implement Cayley retraction on $$\\mathrm{St}(k,n).$$\n","53. Natural gradient on probability simplex with Fisher metric.\n","54. Riemannian gradient descent for covariance estimation on SPD manifold.\n","55. Geodesic on SPD: $$\\gamma(t)=X^{1/2}\\exp\\!\\big(t\\,\\log(X^{-1/2}YX^{-1/2})\\big)X^{1/2}.$$ Compute midpoint.\n","56. Compute Riemannian distance $$d_{\\mathrm{SPD}}(X,Y)=\\|\\log(X^{-1/2}YX^{-1/2})\\|_F.$$\n","57. Projected vs Riemannian gradient comparison for orthonormal $$W.$$\n","58. Trust-region step on sphere: derive subproblem in tangent space.\n","59. Constrained optimization on $$SO(3):$$ align two point clouds (Orthogonal Procrustes).\n","60. EM on manifold: update mean on $$S^2$$ (intrinsic average).\n","\n","---\n","\n","## Module 5 — Information Geometry & Natural Gradient (61–75)\n","\n","61. Derive Fisher information for univariate Gaussian $$\\mathcal{N}(\\mu,\\sigma^2).$$\n","62. Compute Fisher matrix for logistic regression parameters.\n","63. Show natural gradient $$\\tilde{\\nabla} = F^{-1}\\nabla$$ for an exponential family.\n","64. Derive KL divergence local quadratic form via Fisher.\n","65. Compute natural gradient step for softmax regression.\n","66. Show equivalence of NGD with preconditioning by Fisher.\n","67. Derive Fisher for diagonal Gaussian policy (RL).\n","68. Compute Fisher–Rao metric on probability simplex.\n","69. Natural gradient for variational Gaussian posterior.\n","70. Compare Euclidean GD vs NGD on ill-conditioned logistic objective.\n","71. Relate Gauss–Newton to Fisher for least-squares models.\n","72. Derive block-diagonal Fisher approximation (K-FAC idea).\n","73. Compute eigenvalues of Fisher in a 2-parameter Bernoulli model.\n","74. Show invariance of natural gradient under reparameterization.\n","75. Implement a one-step NGD on a toy softmax classifier (symbolic).\n","\n","---\n","\n","## Module 6 — Deep Networks: Jacobians, Hessians, Curvature (76–85)\n","\n","76. Jacobian of a 2-layer MLP $$h_1=\\phi(W_1x+b_1),\\ \\ y=W_2h_1+b_2.$$\n","77. Hessian–vector product (HVP) for MLP using Pearlmutter’s trick.\n","78. Compute spectral norm of Jacobian $$J=\\partial y/\\partial x.$$\n","79. Show gradient explosion via product of layer norms.\n","80. Derive condition for vanishing gradients with sigmoids.\n","81. Curvature of loss at optimum for linear regression (closed-form Hessian).\n","82. HVP for cross-entropy softmax model.\n","83. Empirical Fisher vs true Fisher in classification model.\n","84. Schur complement use in block Hessian of two-layer net.\n","85. Analyze effect of residual connections on Jacobian conditioning.\n","\n","---\n","\n","## Module 7 — Transformers, Attention & Geometry (86–93)\n","\n","86. Backprop through scaled dot-product attention $$\\mathrm{Attn}(Q,K,V)=\\mathrm{softmax}\\!\\big(QK^\\top/\\sqrt{d}\\big)V.$$\n","87. Derive gradient w.r.t. $$Q, K, V$$ in multi-head attention.\n","88. Jacobian of layer norm in Transformer block.\n","89. Backprop through positional encoding (sin/cos) parameters.\n","90. Show softmax temperature effect on gradient scaling.\n","91. Analyze attention map sensitivity (Jacobian of softmax logits).\n","92. Compute curvature (HVP) for attention output w.r.t. query vector.\n","93. Natural gradient step for attention weight matrix under Fisher of softmax.\n","\n","---\n","\n","## Module 8 — Diffusion, Normalization, and Continuous Flows (94–100)\n","\n","94. Backprop through diffusion step $$x_{t+1}=\\alpha_t x_t + \\sigma_t \\epsilon.$$\n","95. Derive score-matching gradient for denoising objective.\n","96. Compute Jacobian trace estimator for continuous normalizing flow (CNF).\n","97. Backprop through ODE solver step (adjoint method outline).\n","98. Fisher information for Gaussian score model.\n","99. Natural gradient update for variance parameter in score-based model.\n","100. Analyze stability of training via Hessian spectrum in diffusion U-Net block.\n"],"metadata":{"id":"yVAUVIvm4Tlt"}}]}